# coding: utf-8
# author: Biao Zhang

import copy
import argparse
import numpy as np
import cPickle as pkl

from collections import OrderedDict


def parseargs():
  msg = "Analyzing the gates learned by CLSR"
  usage = "clsr_gate_analysis.py [<args>] [-h | --help]"
  parser = argparse.ArgumentParser(description=msg, usage=usage)

  parser.add_argument("--lpath", type=str, required=True,
                      help="path to the training/evaluation information of each language pair")
  parser.add_argument("--gpkl", type=str, required=True,
                      help="pkl format gate data, generated by scoring process")
  parser.add_argument("--output", type=str, required=True,
                      help="output path")

  return parser.parse_args()


def load_lang_size_info(lang_size_path):
  """Load training data sizes for each language pair"""
  lang_size_data = []

  for sample in open(lang_size_path, 'r'):
    size, desc = sample.strip().split()
    size = int(size)

    desc_items = desc.strip().split('/')
    if len(desc_items) == 2:
      lang_pair = desc_items[0].split('-')
      lang = lang_pair[0] if lang_pair[1] == 'en' else lang_pair[1]

      lang_size_data.append([size, lang])

  return sorted(lang_size_data, key=lambda x: (x[0], x[1]), reverse=True)


def load_pkled_clsr_gates(gate_path):
  """Load clsr gate data"""
  with open(gate_path, 'rb') as f:
    pkl_data = pkl.load(f)

  # post-processing the pkl data, handling padding
  for i in range(len(pkl_data)):
    # src_str/tgt_str: the tokens of source/target sentences
    # + 1: considering the <eos> token
    src_len = len(pkl_data[i]['src_str']) + 1
    tgt_len = len(pkl_data[i]['tgt_str']) + 1
    for key in pkl_data[i]:
      # encoder side clsr gates (src_len)
      if 'enc' in key:
        pkl_data[i][key] = pkl_data[i][key][:src_len]
      # decoder size clsr gates (tgt_len)
      if 'dec' in key:
        pkl_data[i][key] = pkl_data[i][key][:tgt_len]

  return pkl_data


def split_gates_along_langpairs(pkl_data, name_file, cnt_file):
  """splitting the concatenated dataset for each language pair"""
  names = open(name_file, 'r').readlines()
  cnts = open(cnt_file, 'r').readlines()

  names = [n.strip() for n in names]
  cnts = [int(c.strip().split()[0]) for c in cnts]

  start_idx = 0
  corpus = {}
  for name, cnt in zip(names, cnts):
    end_idx = start_idx + cnt

    name_data = pkl_data[start_idx:end_idx]
    start_idx = end_idx
    lang_pair = name.strip().split('.')[1]

    if 'wmt14' in name:
      lang_pair = name.strip().split('.')[2]

    corpus[lang_pair] = name_data

  assert start_idx == len(pkl_data)
  return corpus


def get_clsr_usage_over_layer_type(lang_clsr_gates):
  """Language specific computation usage, i.e. gate==1, vs encoder or decoder layer type"""

  sample = list(lang_clsr_gates.values())[0][0]
  ordered_sample = OrderedDict(sorted(sample.items(), key=lambda x: x[0]))

  compute_usages = OrderedDict()
  for layer in ordered_sample:
    if 'enc' in layer or 'dec' in layer:
      compute_usages[layer] = []

  # extract gate usage
  for lang_pair in lang_clsr_gates:
    data = lang_clsr_gates[lang_pair]
    for sample in data:
      for layer in compute_usages.keys():
        compute_usages[layer].append(sample[layer])

  # aggregate gate values
  for layer in compute_usages.keys():
    layer_compute_usage = np.mean(np.concatenate(compute_usages[layer], axis=0), axis=0)
    layer_compute_usage_std = np.std(np.concatenate(compute_usages[layer], axis=0), axis=0)
    compute_usages[layer] = (layer_compute_usage, layer_compute_usage_std)

  return compute_usages


def get_clsr_usage_over_lang_frequency(lang_clsr_gates, lang_sizes):
  """Language specific computation usage, i.e. gate==1, vs language pairs, ranking/frequency"""

  sample = list(lang_clsr_gates.values())[0][0]
  ordered_sample = OrderedDict(sorted(sample.items(), key=lambda x: x[0]))

  compute_usages = OrderedDict()
  for layer in ordered_sample:
    if 'enc' in layer or 'dec' in layer:
      compute_usages[layer] = []
  compute_usages['enc'] = []  # all encoder layers
  compute_usages['dec'] = []  # all decoder layers
  compute_usages['all'] = []  # all transformer layers

  lang_compute_usage = {}

  for lang_pair in lang_clsr_gates:
    lang_compute_usage[lang_pair] = copy.copy(compute_usages)

    # gating statistics for this language pair
    # extract all layer information
    data = lang_clsr_gates[lang_pair]
    for sample in data:
      for layer in sample:
        if layer in lang_compute_usage[lang_pair]:
          lang_compute_usage[lang_pair][layer].append(sample[layer])
          lang_compute_usage[lang_pair]['all'].append(sample[layer])
        if 'enc' in layer:
          lang_compute_usage[lang_pair]['enc'].append(sample[layer])
        if 'dec' in layer:
          lang_compute_usage[lang_pair]['dec'].append(sample[layer])

    # directly aggregate these information for this language pair
    for layer in lang_compute_usage[lang_pair].keys():
      data = np.concatenate(lang_compute_usage[lang_pair][layer], axis=0)

      mean_value = np.mean(data, axis=0)
      std_value = np.std(data, axis=0)

      lang_compute_usage[lang_pair][layer] = (mean_value, std_value)

  ranked_lang_usage = []
  for _, lang in lang_sizes:
    lang_pair = 'en2' + lang
    if lang_pair not in lang_compute_usage:
      lang_pair = lang + "2en"
    if lang_pair in lang_compute_usage:
      ranked_lang_usage.append((lang_pair, lang_compute_usage[lang_pair]))

  return ranked_lang_usage


def main(args):
  # loading language sizes
  sorted_lang_sizes = load_lang_size_info(args.lpath+'.size')
  # loading binary gates
  clsr_gates = load_pkled_clsr_gates(args.gpkl)

  # splitting
  lang_clsr_gates = split_gates_along_langpairs(clsr_gates, args.lpath+'.name', args.lpath+'.cnt')
  layer_type_usage = get_clsr_usage_over_layer_type(lang_clsr_gates)
  lang_freq_usage = get_clsr_usage_over_lang_frequency(lang_clsr_gates, sorted_lang_sizes)

  # saving data
  with open(args.output, 'wb') as writer:
    pkl.dump([layer_type_usage, lang_freq_usage], writer)


if __name__ == "__main__":
  args = parseargs()
  main(args)
